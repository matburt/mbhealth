"""Remove analysis comparison tables

Revision ID: 79919bea80cd
Revises: a552c483d3ed
Create Date: 2025-06-30 21:33:51.528811

"""
from collections.abc import Sequence

import sqlalchemy as sa
from sqlalchemy.dialects import sqlite

from alembic import op

# revision identifiers, used by Alembic.
revision: str = '79919bea80cd'
down_revision: str | None = 'a552c483d3ed'
branch_labels: str | Sequence[str] | None = None
depends_on: str | Sequence[str] | None = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    with op.batch_alter_table('analysis_trends', schema=None) as batch_op:
        batch_op.drop_index('ix_analysis_trends_id')

    op.drop_table('analysis_trends')
    with op.batch_alter_table('provider_performance_metrics', schema=None) as batch_op:
        batch_op.drop_index('ix_provider_performance_metrics_id')

    op.drop_table('provider_performance_metrics')
    with op.batch_alter_table('analysis_improvement_suggestions', schema=None) as batch_op:
        batch_op.drop_index('ix_analysis_improvement_suggestions_id')

    op.drop_table('analysis_improvement_suggestions')
    with op.batch_alter_table('analysis_comparisons', schema=None) as batch_op:
        batch_op.drop_index('ix_analysis_comparisons_id')

    op.drop_table('analysis_comparisons')
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('analysis_comparisons',
    sa.Column('id', sa.VARCHAR(), nullable=False),
    sa.Column('user_id', sa.INTEGER(), nullable=False),
    sa.Column('name', sa.VARCHAR(), nullable=False),
    sa.Column('description', sa.TEXT(), nullable=True),
    sa.Column('analysis_ids', sqlite.JSON(), nullable=False),
    sa.Column('comparison_type', sa.VARCHAR(), nullable=False),
    sa.Column('comparison_criteria', sqlite.JSON(), nullable=True),
    sa.Column('comparison_results', sqlite.JSON(), nullable=True),
    sa.Column('key_differences', sqlite.JSON(), nullable=True),
    sa.Column('statistical_insights', sqlite.JSON(), nullable=True),
    sa.Column('trend_analysis', sqlite.JSON(), nullable=True),
    sa.Column('created_at', sa.DATETIME(), nullable=True),
    sa.Column('updated_at', sa.DATETIME(), nullable=True),
    sa.Column('is_shared', sa.BOOLEAN(), nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    with op.batch_alter_table('analysis_comparisons', schema=None) as batch_op:
        batch_op.create_index('ix_analysis_comparisons_id', ['id'], unique=False)

    op.create_table('analysis_improvement_suggestions',
    sa.Column('id', sa.VARCHAR(), nullable=False),
    sa.Column('user_id', sa.INTEGER(), nullable=False),
    sa.Column('analysis_id', sa.INTEGER(), nullable=True),
    sa.Column('suggestion_type', sa.VARCHAR(), nullable=False),
    sa.Column('priority_level', sa.VARCHAR(), nullable=False),
    sa.Column('category', sa.VARCHAR(), nullable=False),
    sa.Column('title', sa.VARCHAR(), nullable=False),
    sa.Column('description', sa.TEXT(), nullable=False),
    sa.Column('detailed_explanation', sa.TEXT(), nullable=True),
    sa.Column('action_steps', sqlite.JSON(), nullable=True),
    sa.Column('expected_improvement', sa.VARCHAR(), nullable=True),
    sa.Column('effort_level', sa.VARCHAR(), nullable=True),
    sa.Column('implementation_time', sa.VARCHAR(), nullable=True),
    sa.Column('supporting_data', sqlite.JSON(), nullable=True),
    sa.Column('related_analyses', sqlite.JSON(), nullable=True),
    sa.Column('success_examples', sqlite.JSON(), nullable=True),
    sa.Column('status', sa.VARCHAR(), nullable=True),
    sa.Column('user_feedback', sa.TEXT(), nullable=True),
    sa.Column('implementation_notes', sa.TEXT(), nullable=True),
    sa.Column('implemented_at', sa.DATETIME(), nullable=True),
    sa.Column('effectiveness_score', sa.FLOAT(), nullable=True),
    sa.Column('follow_up_analysis_ids', sqlite.JSON(), nullable=True),
    sa.Column('created_at', sa.DATETIME(), nullable=True),
    sa.Column('updated_at', sa.DATETIME(), nullable=True),
    sa.Column('expires_at', sa.DATETIME(), nullable=True),
    sa.ForeignKeyConstraint(['analysis_id'], ['ai_analyses.id'], ),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    with op.batch_alter_table('analysis_improvement_suggestions', schema=None) as batch_op:
        batch_op.create_index('ix_analysis_improvement_suggestions_id', ['id'], unique=False)

    op.create_table('provider_performance_metrics',
    sa.Column('id', sa.VARCHAR(), nullable=False),
    sa.Column('provider_id', sa.VARCHAR(), nullable=False),
    sa.Column('user_id', sa.INTEGER(), nullable=False),
    sa.Column('period_start', sa.DATETIME(), nullable=False),
    sa.Column('period_end', sa.DATETIME(), nullable=False),
    sa.Column('period_type', sa.VARCHAR(), nullable=False),
    sa.Column('total_analyses', sa.INTEGER(), nullable=True),
    sa.Column('successful_analyses', sa.INTEGER(), nullable=True),
    sa.Column('failed_analyses', sa.INTEGER(), nullable=True),
    sa.Column('avg_processing_time', sa.FLOAT(), nullable=True),
    sa.Column('total_cost', sa.FLOAT(), nullable=True),
    sa.Column('avg_cost_per_analysis', sa.FLOAT(), nullable=True),
    sa.Column('avg_response_length', sa.FLOAT(), nullable=True),
    sa.Column('avg_token_usage', sa.FLOAT(), nullable=True),
    sa.Column('user_satisfaction_score', sa.FLOAT(), nullable=True),
    sa.Column('analysis_type_breakdown', sqlite.JSON(), nullable=True),
    sa.Column('success_rate', sa.FLOAT(), nullable=True),
    sa.Column('efficiency_score', sa.FLOAT(), nullable=True),
    sa.Column('reliability_score', sa.FLOAT(), nullable=True),
    sa.Column('calculated_at', sa.DATETIME(), nullable=True),
    sa.ForeignKeyConstraint(['provider_id'], ['ai_providers.id'], ),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    with op.batch_alter_table('provider_performance_metrics', schema=None) as batch_op:
        batch_op.create_index('ix_provider_performance_metrics_id', ['id'], unique=False)

    op.create_table('analysis_trends',
    sa.Column('id', sa.VARCHAR(), nullable=False),
    sa.Column('user_id', sa.INTEGER(), nullable=False),
    sa.Column('trend_type', sa.VARCHAR(), nullable=False),
    sa.Column('analysis_type', sa.VARCHAR(), nullable=True),
    sa.Column('metric_focus', sa.VARCHAR(), nullable=True),
    sa.Column('period_start', sa.DATETIME(), nullable=False),
    sa.Column('period_end', sa.DATETIME(), nullable=False),
    sa.Column('data_points_count', sa.INTEGER(), nullable=False),
    sa.Column('trend_direction', sa.VARCHAR(), nullable=True),
    sa.Column('trend_strength', sa.FLOAT(), nullable=True),
    sa.Column('confidence_level', sa.FLOAT(), nullable=True),
    sa.Column('correlation_coefficient', sa.FLOAT(), nullable=True),
    sa.Column('regression_data', sqlite.JSON(), nullable=True),
    sa.Column('seasonal_patterns', sqlite.JSON(), nullable=True),
    sa.Column('outliers', sqlite.JSON(), nullable=True),
    sa.Column('key_insights', sqlite.JSON(), nullable=True),
    sa.Column('improvement_suggestions', sqlite.JSON(), nullable=True),
    sa.Column('next_analysis_suggestions', sqlite.JSON(), nullable=True),
    sa.Column('improvement_percentage', sa.FLOAT(), nullable=True),
    sa.Column('rate_of_change', sa.FLOAT(), nullable=True),
    sa.Column('calculated_at', sa.DATETIME(), nullable=True),
    sa.Column('last_updated', sa.DATETIME(), nullable=True),
    sa.Column('is_significant', sa.BOOLEAN(), nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    with op.batch_alter_table('analysis_trends', schema=None) as batch_op:
        batch_op.create_index('ix_analysis_trends_id', ['id'], unique=False)

    # ### end Alembic commands ###
